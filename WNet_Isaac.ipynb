{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/miniforge3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from glob import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import src.wnet as wnet\n",
    "\n",
    "device = 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Dataset\n",
    "class BSDS500_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, subset):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_paths = glob(os.path.join(root_dir, 'BSDS500', 'data', 'images', subset, '*.jpg'))\n",
    "        self.image_sizes = [np.take(torchvision.io.read_image(img_path).shape, [1,2]) for img_path in self.image_paths]\n",
    "        self.image_size = (224, 224)\n",
    "        \n",
    "        self.max_image = None\n",
    "        self.min_image = None\n",
    "        for i in range(self.__len__()):\n",
    "            image = torchvision.io.read_image(self.image_paths[i]).float()\n",
    "            image = torchvision.transforms.functional.resize(image, self.image_size, interpolation=torchvision.transforms.functional.InterpolationMode.BILINEAR)\n",
    "            self.max_image = torch.max(self.max_image, image) if self.max_image is not None else image\n",
    "            self.min_image = torch.min(self.min_image, image) if self.min_image is not None else image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def size(self, idx=None):\n",
    "        s = [self.__len__(), 3, self.image_size[0], self.image_size[1]]\n",
    "        if idx is not None:\n",
    "            s = s[idx]\n",
    "        else:\n",
    "            s = torch.Size(s)\n",
    "        return s\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = torchvision.io.read_image(img_path)\n",
    "        image = image.float()\n",
    "        image = torchvision.transforms.functional.resize(image, self.image_size, interpolation=torchvision.transforms.functional.InterpolationMode.BILINEAR)\n",
    "        image = (image - self.min_image) / (self.max_image - self.min_image)\n",
    "        return image.to(device)\n",
    "\n",
    "class BSDS500():\n",
    "    def __init__(self, root_dir, batch_size):\n",
    "        self.trainset = BSDS500_dataset(root_dir, 'train')\n",
    "        self.trainloader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=True)\n",
    "        self.valset = BSDS500_dataset(root_dir, 'val')\n",
    "        self.valloader = torch.utils.data.DataLoader(self.valset, batch_size=batch_size, shuffle=False)\n",
    "        self.testset = BSDS500_dataset(root_dir, 'test')\n",
    "        self.testloader = torch.utils.data.DataLoader(self.testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    def get_trainloader(self):\n",
    "        return self.trainloader\n",
    "    \n",
    "    def get_valloader(self):\n",
    "        return self.valloader\n",
    "\n",
    "    def get_testloader(self):\n",
    "        return self.testloader\n",
    "\n",
    "\n",
    "class PascalVOC_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, subset):\n",
    "        self.root_dir = root_dir\n",
    "        with open(os.path.join(root_dir, 'VOC2012', 'ImageSets', 'Segmentation', subset + '.txt'), 'r') as f:\n",
    "            self.image_names = f.read().split(\"\\n\")\n",
    "        self.image_paths = [os.path.join(root_dir, 'VOC2012', 'JPEGImages', image_name + '.jpg') for image_name in self.image_names if image_name != '']\n",
    "        self.image_sizes = [np.take(torchvision.io.read_image(img_path).shape, [1,2]) for img_path in self.image_paths]\n",
    "        self.image_size = (224, 224)\n",
    "        self.max_image = None\n",
    "        self.min_image = None\n",
    "        for i in range(self.__len__()):\n",
    "            image = torchvision.io.read_image(self.image_paths[i]).float()\n",
    "            image = torchvision.transforms.functional.resize(image, self.image_size, interpolation=torchvision.transforms.functional.InterpolationMode.BILINEAR)\n",
    "            self.max_image = torch.max(self.max_image, image) if self.max_image is not None else image\n",
    "            self.min_image = torch.min(self.min_image, image) if self.min_image is not None else image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def size(self, idx=None):\n",
    "        s = [self.__len__(), 3, self.image_size[0], self.image_size[1]]\n",
    "        if idx is not None:\n",
    "            s = s[idx]\n",
    "        else:\n",
    "            s = torch.Size(s)\n",
    "        return s\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = torchvision.io.read_image(img_path)\n",
    "        image = image.float()\n",
    "        image = torchvision.transforms.functional.resize(image, self.image_size, interpolation=torchvision.transforms.functional.InterpolationMode.BILINEAR)\n",
    "        image = (image - self.min_image) / (self.max_image - self.min_image)\n",
    "        return image.to(device)\n",
    "\n",
    "class PascalVOC():\n",
    "    def __init__(self, root_dir, batch_size):\n",
    "        self.trainset = PascalVOC_dataset(root_dir, 'train')\n",
    "        self.trainloader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=True)\n",
    "        self.valset = PascalVOC_dataset(root_dir, 'val')\n",
    "        self.valloader = torch.utils.data.DataLoader(self.valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    def get_trainloader(self):\n",
    "        return self.trainloader\n",
    "    \n",
    "    def get_valloader(self):\n",
    "        return self.valloader\n",
    "\n",
    "if device == \"mps\":\n",
    "    batch_size = 16\n",
    "else:\n",
    "    batch_size = 8\n",
    "\n",
    "pascal = PascalVOC('../VOCdevkit', batch_size)\n",
    "X_train = pascal.get_trainloader()\n",
    "y_train = pascal.get_trainloader()\n",
    "X_val = pascal.get_valloader()\n",
    "y_val = pascal.get_valloader()\n",
    "bsds = BSDS500('../BSDS500', batch_size)\n",
    "X_test = bsds.get_testloader()\n",
    "y_test = bsds.get_testloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN:   1%|          | 1/92 [00:00<00:13,  6.68it/s]/Users/isaac/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1665990514504/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "TRAIN:   7%|▋         | 6/92 [00:10<02:26,  1.71s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLoaded checkpoint from \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(checkpoint_path))\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m train:\n\u001b[0;32m---> 23\u001b[0m     net\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     24\u001b[0m         X_train, y_train,\n\u001b[1;32m     25\u001b[0m         X_val, y_val,\n\u001b[1;32m     26\u001b[0m         epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     27\u001b[0m         learn_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m         weight_decay\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     date \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39m\u001b[39m__str__\u001b[39m()\n\u001b[1;32m     31\u001b[0m     date \u001b[39m=\u001b[39m date[:\u001b[39m16\u001b[39m]\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/CIS 520/Final Project/WNet_Isaac/src/network.py:135\u001b[0m, in \u001b[0;36mNetwork.fit\u001b[0;34m(self, x_train, y_train, x_val, y_val, learn_rate, weight_decay, epochs, callbacks, plot)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m    134\u001b[0m     num_epochs \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 135\u001b[0m     tr_loss, va_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(x_train, y_train, x_val, y_val, optimizer)\n\u001b[1;32m    136\u001b[0m     tr_losses\u001b[39m.\u001b[39mappend(tr_loss)\n\u001b[1;32m    137\u001b[0m     va_losses\u001b[39m.\u001b[39mappend(va_loss)\n",
      "File \u001b[0;32m~/Desktop/CIS 520/Final Project/WNet_Isaac/src/network.py:89\u001b[0m, in \u001b[0;36mNetwork.step\u001b[0;34m(self, x_train, y_train, x_val, y_val, optimizer)\u001b[0m\n\u001b[1;32m     87\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     88\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 89\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mitem() \u001b[39m/\u001b[39m train_batches\n\u001b[1;32m     91\u001b[0m progress_bar\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m x_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m y_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import src.wnet as wnet\n",
    "import src.network as network\n",
    "import src.loss as loss\n",
    "loss = reload(loss)\n",
    "network = reload(network)\n",
    "wnet = reload(wnet)\n",
    "\n",
    "train = True\n",
    "epochs = 10\n",
    "use_checkpoint = False\n",
    "checkpoint_path = 'models/wnet-2022-12-02-02-50.pt'\n",
    "\n",
    "net = wnet.WNet(device_type=device)\n",
    "net.to(device)\n",
    "if use_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    net.to(device)\n",
    "    print('Loaded checkpoint from {}'.format(checkpoint_path))\n",
    "\n",
    "if train:\n",
    "    net.fit(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs=epochs,\n",
    "        learn_rate=1e-3,\n",
    "        weight_decay=1e-5\n",
    "    )\n",
    "    date = datetime.now().__str__()\n",
    "    date = date[:16].replace(':', '-').replace(' ', '-')\n",
    "    torch.save({\n",
    "                'epoch': epochs,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                }, f'models/wnet-{date}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [08:02, 19.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import utils.visualize as visualize\n",
    "from src.crf import crf_fit_predict, crf_batch_fit_predict\n",
    "import tqdm\n",
    "\n",
    "all_inputs = []\n",
    "all_masks = []\n",
    "all_network_masks = []\n",
    "for i,batch in tqdm.tqdm(enumerate(X_test)):\n",
    "  inputs = batch\n",
    "  mask, outputs = net.forward(inputs)\n",
    "  inputs = inputs.detach().cpu().numpy()\n",
    "  outputs = outputs.detach().cpu().numpy()\n",
    "  mask = mask.detach().cpu().numpy()\n",
    "  crf_mask = crf_batch_fit_predict(mask, inputs)\n",
    "  for j in range(inputs.shape[0]):\n",
    "    all_inputs.append(inputs[j])\n",
    "    all_network_masks.append(mask[j])\n",
    "    all_masks.append(crf_mask[j])\n",
    "    \n",
    "all_masks = np.array(all_masks)\n",
    "all_inputs = np.array(all_inputs)\n",
    "all_network_masks = np.array(all_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert segmentation mask to RGB with colors averaged from the image\n",
    "def mask_to_rgb(mask, image):\n",
    "    mask = mask.argmax(0)\n",
    "    image = image.transpose(1,2,0)\n",
    "    mask_rgb = np.zeros(image.shape)\n",
    "    for i in range(1, mask.max()+1):\n",
    "        mask_rgb[mask==i] = image[mask==i].mean(0)\n",
    "    return mask_rgb\n",
    "\n",
    "# edge detection using sobel filter\n",
    "def sobel_edge_detection(image):\n",
    "    image = skimage.color.rgb2gray(image)\n",
    "    edges = skimage.filters.sobel(image)\n",
    "    return edges\n",
    "\n",
    "# segmentation mask to csv\n",
    "def mask_to_csv(mask):\n",
    "    mask = mask.argmax(0)\n",
    "    print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12573/2182552978.py:10: RuntimeWarning: Mean of empty slice.\n",
      "  mask_rgb[mask==i] = image[mask==i].mean(0)\n",
      "/home/remote/anaconda3/envs/torch/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "for i in range(all_masks.shape[0]):\n",
    "    mask = mask_to_rgb(all_masks[i], all_inputs[i])\n",
    "    mask = torch.from_numpy(mask).permute(2,0,1)\n",
    "    id = dataset.testset.image_paths[i].split(\"/\")[-1].split(\".\")[0]\n",
    "    mask = torchvision.transforms.functional.resize(mask, list(dataset.testset.image_sizes[i]), interpolation=torchvision.transforms.functional.InterpolationMode.NEAREST)\n",
    "    torchvision.utils.save_image(mask, f'outputs/png/{id}.png')\n",
    "    \n",
    "    # mask = mask_to_rgb(all_masks[i], all_inputs[i])\n",
    "    mask = (skimage.color.rgb2gray(mask.permute(1,2,0)) * 255).round()\n",
    "    colors = np.unique(mask)\n",
    "    for i,color in enumerate(colors):\n",
    "        mask[mask==color] = i\n",
    "    np.savetxt(f'outputs/csv/{id}.csv', mask, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:18<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average (averaged over gt segmentations) mIOU:  0.11433732975607845\n",
      "Max (over gt segmentations) mIOU:  0.2720017964656836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation import calculate_mIOU\n",
    "\n",
    "average_mIOU, max_mIOU = calculate_mIOU(\"../BSDS500/gt\", \"outputs/csv\")\n",
    "print(\"Average (averaged over gt segmentations) mIOU: \", average_mIOU)\n",
    "print(\"Max (over gt segmentations) mIOU: \", max_mIOU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "394d24a1087ab98538c556b121a84e46182f365585e86aa7ee7d846ee7e5fae4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
